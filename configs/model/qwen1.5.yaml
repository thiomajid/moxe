architectures:
  - Qwen2MoeForCausalLM
attention_dropout: 0.0
bos_token_id: 151643
decoder_sparse_step: 1
eos_token_id: 151643
hidden_act: silu
hidden_size: 2048
initializer_range: 0.02
intermediate_size: 5632
max_position_embeddings: 8192
max_window_layers: 21
mlp_only_layers: []
model_type: qwen2_moe
moe_intermediate_size: 1408
norm_topk_prob: false
num_attention_heads: 16
num_experts: 60
num_experts_per_tok: 4
num_hidden_layers: 24
num_key_value_heads: 16
output_router_logits: false
rms_norm_eps: 1.0e-06
rope_scaling: null
rope_theta: 1000000.0
router_aux_loss_coef: 0.001
shared_expert_intermediate_size: 5632
sliding_window: null
tie_word_embeddings: false
torch_dtype: bfloat16
transformers_version: 4.45.2
use_cache: true
use_sliding_window: false
vocab_size: 151936
