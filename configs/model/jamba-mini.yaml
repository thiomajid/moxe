architectures:
- JambaForCausalLM
attention_dropout: 0.0
attn_layer_offset: 4
attn_layer_period: 8
bos_token_id: 1
eos_token_id:
- 2
- 518
expert_layer_offset: 1
expert_layer_period: 2
hidden_act: silu
hidden_size: 4096
initializer_range: 0.02
intermediate_size: 14336
mamba_conv_bias: true
mamba_d_conv: 4
mamba_d_state: 16
mamba_dt_rank: 256
mamba_expand: 2
mamba_proj_bias: false
max_position_embeddings: 262144
model_type: jamba
num_attention_heads: 32
num_experts: 16
num_experts_per_tok: 2
num_hidden_layers: 32
num_key_value_heads: 8
num_logits_to_keep: 1
output_router_logits: false
pad_token_id: 0
rms_norm_eps: 1.0e-06
router_aux_loss_coef: 0.001
sliding_window: null
tie_word_embeddings: false
torch_dtype: bfloat16
transformers_version: 4.45.2
use_cache: true
use_mamba_kernels: true
vocab_size: 65536
