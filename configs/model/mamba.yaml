bos_token_id: 0
conv_kernel: 4
eos_token_id: 0
expand: 2
hidden_act: silu
hidden_size: 768
initializer_range: 0.1
intermediate_size: 1536
layer_norm_epsilon: 1.0e-05
model_type: mamba
num_hidden_layers: 32
pad_token_id: 0
rescale_prenorm_residual: false
residual_in_fp32: true
state_size: 16
time_step_floor: 0.0001
time_step_init_scheme: random
time_step_max: 0.1
time_step_min: 0.001
time_step_rank: 48
time_step_scale: 1.0
transformers_version: 4.45.2
use_bias: false
use_cache: true
use_conv_bias: true
use_mambapy: false
vocab_size: 50280
