architectures:
  - LlamaForCausalLM
attention_bias: false
attention_dropout: 0.0
bos_token_id: 1
eos_token_id: 2
head_dim: 64
hidden_act: silu
hidden_size: 2048
initializer_range: 0.02
intermediate_size: 8192
max_position_embeddings: 8192
mlp_bias: false
model_type: llama
num_attention_heads: 32
num_hidden_layers: 24
num_key_value_heads: 32
pad_token_id: 2
pretraining_tp: 1
rms_norm_eps: 1.0e-05
rope_scaling: null
rope_theta: 130000
tie_word_embeddings: true
torch_dtype: bfloat16
transformers.js_config:
  dtype: q4
  kv_cache_dtype:
    fp16: float16
    q4f16: float16
  use_external_data_format:
    model.onnx: true
    model_fp16.onnx: true
transformers_version: 4.45.2
use_cache: true
vocab_size: 49152
