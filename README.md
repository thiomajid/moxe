# MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling

This repository contains the code for modeling and training a MoxE from our paper: [MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling](https://www.arxiv.org/abs/2505.01459).

If you use this code, please cite our paper:

```bibtex
@misc{thiombiano2025moxemixturexlstmexperts,
      title={MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling}, 
      author={Abdoul Majid O. Thiombiano and Brahim Hnich and Ali Ben Mrad and Mohamed Wiem Mkaouer},
      year={2025},
      eprint={2505.01459},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.01459}, 
}
```

## References

```bibtex
@inproceedings{beck:24xlstm,
      title={xLSTM: Extended Long Short-Term Memory}, 
      author={Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      booktitle = {Thirty-eighth Conference on Neural Information Processing Systems},
      year={2024},
      url={https://arxiv.org/abs/2405.04517}, 
}

@article{beck:25xlstm7b,
  title        = {{xLSTM 7B}: A Recurrent LLM for Fast and Efficient Inference},
  author       = {Maximilian Beck and Korbinian Pöppel and Phillip Lippe and Richard Kurle and Patrick M. Blies and Günter Klambauer and Sebastian Böck and Sepp Hochreiter},
  year         = {2025},
  volume       = {2503.13427},
  journal      = {arXiv},
  primaryclass = {cs.LG},
  url          = {https://arxiv.org/abs/2503.13427}
}

@misc{xlstm-jax,
  title={xLSTM-jax},
  author={NXAI GmbH},
  year={2024},
  url={https://github.com/NX-AI/xlstm-jax/},
}
```